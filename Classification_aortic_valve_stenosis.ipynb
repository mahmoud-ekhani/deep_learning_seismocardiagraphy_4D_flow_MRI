{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21f34c14",
   "metadata": {},
   "source": [
    "# Classification of valve condition into TAV, BAV, MAV, and AS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa6ab2b",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d9fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import callbacks as CB\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Rescaling \n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import RandomFlip\n",
    "from tensorflow.keras.layers import RandomRotation\n",
    "from tensorflow.keras.layers import RandomZoom\n",
    "from tensorflow.keras.layers import add\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow.keras as keras\n",
    "import scipy.io\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "# specify the gpu to be utilized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf40e9",
   "metadata": {},
   "source": [
    "## Create the multi-layer perceptron (MLP) and convolutional neural network (CNN) models\n",
    "\n",
    "The input of the MLP is the demographic information of the subjects, including their weight, height, age, and gender. The MLLP model is composed of two fully-connected dense layers. All three continuous attributes are min-max normalized, and the participants' gender are one-hot encoded (i.e. female = 1, male = 0) before being fed into the MLP.  \n",
    "    \n",
    "The input of the CNN model is the scalograms of the recorded SCG pulses. The CNN architecture consisted of a stack of five convolutional blocks, with each block containing a sequence of Conv2D, ReLU (rectified linear unit) activation, batch normalization, and max pooling layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ad2f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "def create_mlp(dim):\n",
    "    # define the Multilayer Perceptron (MLP) network:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8,input_dim=dim,activation=\"relu\"))\n",
    "    model.add(Dense(4,activation=\"relu\"))\n",
    "    # return the model\n",
    "    return model\n",
    "\n",
    "# CNN model\n",
    "def create_cnn(image_size=(256,256,1),filters=(64,128,256,512),augmentation=False):\n",
    "    # define the model input\n",
    "    inputs = Input(shape=image_size)\n",
    "    # data augmentation\n",
    "    if augmentation:\n",
    "        x = RandomFlip(\"horizontal\")(inputs)\n",
    "        x = RandomRotation(0.1)(x)\n",
    "        x = RandomZoom(0.2)(x)\n",
    "    # loop over filters\n",
    "    for i, filter in enumerate(filters):\n",
    "        # if this is the first layer, set the input appropriately\n",
    "        if i == 0 and not augmentation:\n",
    "            x = inputs\n",
    "        # CONV => RELU => BN => POOL\n",
    "        x = Conv2D(filter,kernel_size=3,padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=-1)(x)\n",
    "        x = MaxPooling2D(pool_size=2)(x)\n",
    "    # flatten the volume, then FC=>RELU=>BN=>DROPOUT\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(16)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    # apply anothe layer of FC layer, this one matches the number of nodes\n",
    "    # coming out of the MLP \n",
    "    x = Dense(4)(x)\n",
    "    outputs = Activation(\"relu\")(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.summary()\n",
    "    # return the CNN model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f66025",
   "metadata": {},
   "source": [
    "## Define a function to load the .mat files \n",
    "\n",
    "The .mat files contain three type of information:\n",
    "\n",
    "    The \"encAsValve\" contains the one-hot encoded aortic valve condition classes\n",
    "    The \"attr\" contains the demographic attributes of the subjects\n",
    "    The \"scalogram\" contains the scalograms of the seismocardiogram pulses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee30a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mat_file(mat_file):\n",
    "    f = h5py.File(mat_file+'.mat','r')\n",
    "    variables = f.items()\n",
    "    for var in variables:\n",
    "        name = var[0]\n",
    "        data = var[1]\n",
    "        if type(data) is not h5py.Dataset:\n",
    "            continue\n",
    "        if str(name)==\"encAsValve\":\n",
    "            encAsValve = np.array(data,dtype=np.float32)\n",
    "            encAsValve = np.transpose(encAsValve,(1,0))\n",
    "        elif str(name)==\"attr\":\n",
    "            attr = np.array(data,dtype=np.float32)\n",
    "            attr = np.transpose(attr,(1,0))\n",
    "        elif str(name)==\"scalogram\":\n",
    "            scg = np.array(data,dtype=np.float32)\n",
    "            scg = np.transpose(scg,(2,1,0))\n",
    "            sh  = np.shape(scg)\n",
    "            scg = np.resize(scg,(sh[0],sh[1],sh[2],1))\n",
    "    return scg, attr, encAsValve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0987bf2a",
   "metadata": {},
   "source": [
    "## Run the model for multiple iterations - leave-subject-out cross validation\n",
    "\n",
    "To evaluate the performance of the models, we use a leave-subject-out cross-validation approach. This means that for each iteration, we use 50% of the available SCG pulses (N = 6249) for training the model and reserve the remaining 50% for testing. Importantly, the SCG pulses belonging to each subject are only included in either the training or test set, but not both, in order to avoid any potential bias in the performance metrics. We repeate this process for 10 iterations, randomly selecting different subjects to be included in the training and test sets at each trial. This allows us to verify that the reported performance metrics are not influenced by any specific patterns in the distribution of the training and test sets. For each iteration, the model is trained for 150 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16e3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a callback function to monitor the model's performance over the validation set\n",
    "callback = CB.ModelCheckpoint(\"mixed_mdl_class.keras\",\n",
    "                             save_best_only=True, \n",
    "                              monitor=\"val_loss\")\n",
    "# tensorboard = CB.TensorBoard(log_dir='/home/mem1342/Projects/quest_scg_regression')\n",
    "history = list()\n",
    "for iterations in range(1,11):\n",
    "    # read the training set\n",
    "    scg_train,atr_train,encAsValve_train=load_mat_file(os.path.join('scg_dataset/50-50',\n",
    "                                                                     'train_cv_'+str(iterations)))\n",
    "    # return the predictors of the training set\n",
    "    train_x = [atr_train,scg_train]\n",
    "    # return the labels of training set\n",
    "    train_y = encAsValve_train\n",
    "    # read the test set\n",
    "    scg_test,atr_test,encAsValve_test=load_mat_file(os.path.join('scg_dataset/50-50',\n",
    "                                                                  'test_cv_'+str(iterations)))\n",
    "    # return the predictors of test set\n",
    "    test_x = [atr_test,scg_test]\n",
    "    # return the labels of test set n\n",
    "    test_y = encAsValve_test\n",
    "    # create a multi-layer perceptron model based on subject attributes\n",
    "    mlp = create_mlp(atr_train.shape[1])\n",
    "    # create a cnn model based on scalograms\n",
    "    cnn = create_cnn(image_size=(256,256,1),filters=[4,8,16,32,128],augmentation=False)\n",
    "    # combine created mlp and cnn models\n",
    "    combInp = keras.layers.concatenate([mlp.output,cnn.output])\n",
    "    x = keras.layers.Dense(4,activation=\"relu\")(combInp)\n",
    "    # return the output associated with combination of aortic stenosis and the valve condition\n",
    "    y1 = keras.layers.Dense(4,activation=\"softmax\",name='aortic_stenosis_valve')(x)\n",
    "    model = keras.Model(inputs=[mlp.input,cnn.input],outputs=y1)\n",
    "    # compile the mixture model using mean absolute percentage error as loss function\n",
    "    opt = Adam(learning_rate=1e-3, decay=1e-3 / 100)\n",
    "    model.compile(optimizer=opt, \n",
    "              loss={'aortic_stenosis_valve': 'categorical_crossentropy'},\n",
    "              metrics={'aortic_stenosis_valve': 'accuracy'})\n",
    "    # train the model\n",
    "    hist = model.fit(x=train_x, \n",
    "                  y=train_y,\n",
    "                  validation_data = (test_x, test_y),\n",
    "                  epochs=150,\n",
    "                  batch_size=32, \n",
    "                  verbose=2,\n",
    "                  callbacks=[callback])\n",
    "    history.append(hist)\n",
    "    # load the model at the epoch with the lowest error over the validation set\n",
    "    model = tf.keras.models.load_model(\"mixed_mdl_class.keras\")\n",
    "    # predict the vmax in test set\n",
    "    preds = model.predict(test_x,verbose=2)\n",
    "    # return the true and predicted aortic stenosis condition\n",
    "    pred = np.array(preds)\n",
    "    asValve = list() \n",
    "    asValve.append(encAsValve_test)\n",
    "    asValve.append(pred)\n",
    "    filename1 = os.path.join('/home/mem1342/Projects/quest_scg_regression/results',\n",
    "                             'cv_'+str(iterations)+'.mat')\n",
    "    scipy.io.savemat(filename1,{'as_valve':asValve})\n",
    "    # save the roc curves\n",
    "    roc_auc = np.zeros(shape=(4,))\n",
    "    for i in range(4):\n",
    "        roc_auc[i] = roc_auc_score(encAsValve_test[:,i],pred[:,i])\n",
    "    print(f\"The mean ROC AUC value: {np.mean(roc_auc): 0.2f} \")\n",
    "    tr_acc = hist.history[\"accuracy\"]\n",
    "    val_acc = hist.history[\"val_accuracy\"]\n",
    "    epochs = range(1,len(tr_acc)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs,tr_acc,'bo',label=\"Training\")\n",
    "    plt.plot(epochs,val_acc,'ro',label=\"Validation\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f\"The mean ROC AUC value: {np.mean(roc_auc): 0.2f} \")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
